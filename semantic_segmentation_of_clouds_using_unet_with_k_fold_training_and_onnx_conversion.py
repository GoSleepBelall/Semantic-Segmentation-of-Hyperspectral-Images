# -*- coding: utf-8 -*-
"""Semantic_Segmentation_of_Clouds_using_UNET with K-fold training and ONNX conversion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bk6mO7In9ZqCXo8k-i2b0SWAg2OAJVVa

# Semantic Segmentation of Clouds using UNet
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Import Dependencies'
These libraries are used to perform semantic segmentation of clouds using UNet.
"""

import numpy as np
import tensorflow as tf
import os
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda
from keras.optimizers import Adam
from keras.layers import Activation, MaxPool2D, Concatenate
from sklearn.model_selection import KFold
import pandas as pd

"""This is a chunk of code used to clear RAM in case of memory issues."""

#Clear RAM
import gc
gc.collect()

"""## Configure GPU
To use the maximum capacity of Google Colab, We have to use cloud GPU, In case if you haven't setup GPU, go to runtime type, and add GPU as hardware accelerator.
"""

device = '/GPU:0' if tf.config.experimental.list_physical_devices('GPU') else '/CPU:0'
print(device)

"""## Import Dataset
All operations related to Dataset was performed in this section including importing, exporting, tiling.

### Load presaved dataset
In case if you've already stored data somewhere in google drive, you can load the files here.
"""

path = '/content/drive/MyDrive/Colab Notebooks/ZAIT/DATA/subscenes/S2A_MSIL1C_20180101T010721_N0206_R045_T53HLD_20180101T041600.npy'
X = np.load(path)

path = '/content/drive/MyDrive/Colab Notebooks/ZAIT/DATA/masks/S2A_MSIL1C_20180101T010721_N0206_R045_T53HLD_20180101T041600.npy'
y = np.load(path)

"""## Perform Tiling on Dataset
Tiling is the concept of breaking your large image into smaller portions in a way that no data is lost. We've performed overlapping tiling with an offset of 0.5 to get desired results.

### Define Limits
These Limits are used to define how many files the program will read, In case of memory issues, user can take a chunk of files by defining the low and high points. Both points represents the file number present in folder.
"""

low = 0
high= 10 #500

"""### Load Masks and Tile
In this section, Numpy files of Masks dataset (labels) are loaded from disk, and are passed through the process of tiling.
"""

# Function to perform tiling on masks provided
def tile_img(img, size, overlap=0.5):
    """
    Function to tile an input image into overlapping tiles
    """
    tiles = []
    if len(img.shape) == 2: 
      height, width = img.shape
    elif len(img.shape) == 3:
       height, width, channel = img.shape
    tile_h, tile_w = size[:2]
    offset_h = int(tile_h * overlap)
    offset_w = int(tile_w * overlap)
    for i in range(0, height-tile_h+1, offset_h):
        for j in range(0, width-tile_w+1, offset_w):
            tiles.append(img[i:i+tile_h, j:j+tile_w])
    return tiles

def read_masks_and_tile(folder_path):
    """
    Function to read all .npy files present in a folder and tile them
    """
    images = []
    for count, file in enumerate(os.listdir(folder_path)):
        if file.endswith('.npy') and (count >= low and count < high):
            img = np.load(os.path.join(folder_path, file))
            # Take only cloud channel
            img = tf.convert_to_tensor(img[:, :, 1]) #img shape (1022,1022)
            tiles = tile_img(img, (224, 224))  
            images.extend(tiles)
    return images

folder_path = '/content/drive/MyDrive/DATA/masks'
with tf.device(device):
  masks = read_masks_and_tile(folder_path)
y = np.array(masks)

"""### Load Subscenes and Tile
In this section, Numpy files of Subscenes dataset (labels) are loaded from disk, and are passed through the process of tiling.
"""

def read_subscene_and_tile(folder_path):
    """
    Function to read all .npy files present in a folder and tile them
    """
    images = []
    count= 0
    for file in os.listdir(folder_path):
        if file.endswith('.npy') and (count >= low and count < high):
            img = np.load(os.path.join(folder_path, file))
            img = tf.convert_to_tensor(img, dtype=tf.float32)
            img = tf.concat((img[..., :3], img[..., 7:8]), axis=-1)
            tiles = tile_img(img, (224, 224, 4))
            images.extend(tiles)
        count = count+1
    return images


# Read all .npy files present in the folder images and convert them in tiles
folder_path = '/content/drive/MyDrive/DATA/subscenes'
with tf.device(device):
  subscenes = read_subscene_and_tile(folder_path)

X = np.array(subscenes)

"""### Change Shape of Dataset"""

X = np.transpose(X, (0,3,1,2))

"""### Save Datasets
Once the data is processed and tiled, We can save in google drive
"""

path = '/content/drive/MyDrive/Colab Notebooks/ZAIT/DATA/subscenes.npy'
np.save(path, X)

path = '/content/drive/MyDrive/Colab Notebooks/ZAIT/DATA/masks.npy'
np.save(path, y)

"""## Check Shape of Data
In order to confirm shape of data loaded or processed, We can call these two cells below:
"""

np.shape(X)

np.shape(y)

"""## Data Visualization
Data of Subscenes and Labels in the form of tiles are visualized in this section:

###! Important
Data Will not be visualized if datset is transposed and is processed according to the required shape of model
"""

# Must RUN ONCE if you need to visualize the data
custom_cmap = ListedColormap(['black', 'white'])

# Change the variable to print any image
imgnum = 68
fig, axs = plt.subplots(1, 6, figsize=(20,10))
axs[0].imshow(X[imgnum,:,:,0:3])
axs[1].imshow(X[imgnum,:,:,1])
axs[2].imshow(X[imgnum,:,:,2])
axs[3].imshow(X[imgnum,:,:,0])
axs[4].imshow(X[imgnum,:,:,3])
axs[5].imshow(y[imgnum,...], cmap=custom_cmap, vmin=0, vmax=1)

plt.show()
#White represents the CLOUD = 1
#Black represents the CLEAR = 0

"""
# FOR CONFIRMATION OF DATA
# To print the start array in binary
for i in range(30):
  for j in range(224):
    if y[imgnum][i][j] == True:
      print(1, end="")
    else:
      print(0, end="")
  print()
"""

"""## Build Model
UNet is build in this section.
The original model proposed in the paper do not contain Batch Normalization, but for optimal solution to our problem, we have used Batch Normalization in convolution block of the model used in both encoder and decoder.
"""

def conv_block(input, num_filters):
    x = Conv2D(filters= num_filters, kernel_size= 3, padding="same")(input)
    x = BatchNormalization()(x)    
    x = Activation("relu")(x)

    x = Conv2D(filters= num_filters, kernel_size= 3, padding="same")(x)
    x = BatchNormalization()(x)  
    x = Activation("relu")(x)
    return x

#Encoder block: Conv block followed by maxpooling
def encoder_block(input, num_filters):
    x = conv_block(input, num_filters)
    p = MaxPool2D((2, 2))(x)
    return x, p   

#Decoder block
#skip features gets input from encoder for concatenation
def decoder_block(input, skip_features, num_filters):
    x = Conv2DTranspose(filters= num_filters, kernel_size= (2, 2), strides=2, padding="same")(input)
    x = Concatenate()([x, skip_features])
    x = conv_block(x, num_filters)
    return x

#Build Unet using the blocks
def build_unet(input_shape):
    inputs = Input(input_shape)

    s1, p1 = encoder_block(inputs, 64)
    s2, p2 = encoder_block(p1, 128)
    s3, p3 = encoder_block(p2, 256)
    s4, p4 = encoder_block(p3, 512)

    b1 = conv_block(p4, 1024) #Bridge

    d1 = decoder_block(b1, s4, 512)
    d2 = decoder_block(d1, s3, 256)
    d3 = decoder_block(d2, s2, 128)
    d4 = decoder_block(d3, s1, 64)
    outputs = Conv2D(filters=1, kernel_size=1, padding="same", activation='sigmoid')(d4)
    #outputs = tf.keras.backend.round(outputs)  # Round the output to 0 or 1
    model = Model(inputs, outputs, name="U-Net")
    return model

segmentation_model = build_unet((224,224,4))

"""## Load the Model
In case if you already have a pretrained model present in google drive, give a path and load the model in colab.
"""

path = ('/content/drive/MyDrive/saved_model.pb')
segmentation_model = tf.keras.models.load_model(path)

"""To print the summary of model: run the following cell:"""

segmentation_model.summary()

"""## Train the Model using K-fold
The model was trained and evaluated in this process
"""

# Train the model
k = 2
kf = KFold(n_splits=k)
histories = []
with tf.device(device):
  segmentation_model.compile(optimizer=Adam(learning_rate = 1e-3), loss='binary_crossentropy', metrics=['accuracy'])
  for train_index, val_index in kf.split(X):
      X_train, X_val = X[train_index], X[val_index]
      y_train, y_val = y[train_index], y[val_index]

      history = segmentation_model.fit(X_train, y_train, epochs=2, batch_size=32, validation_data=(X_val, y_val))
      histories.append(history)

"""## Plot the histories
Plot the loss, accuracy while traininga as well as in validation.
"""

pd.DataFrame(histories[0].history).plot(figsize=(8,5))
plt.show()
#print(history.history)

"""## Check Inference"""

#out = np.where(segmentation_model(X[0,...] <0.5,0,1))
# Multiple Inferences
out = segmentation_model(X[0:70,...])
out = np.squeeze(out)
out = np.where(out<0.5,0,1)
print(np.shape(out[68]))
print(out[68])
plt.imshow(out[68],  cmap=custom_cmap, vmin=0, vmax=1)
plt.show()

"""## Save the model
If you're done training your model and its working fine, you can save the model using the following code:
"""

# Save the trained model for future use
segmentation_model.save('/content/drive/MyDrive/saved_model.pb')

"""## Validate The Model"""

# Evaluate the model on the validation data
val_loss, val_acc = segmentation_model.evaluate(X, y, verbose=0)
print("Validation Loss:", val_loss)
print("Validation Accuracy:", val_acc)

"""## Convert to ONNX"""

import tensorflow as tf
import tf2onnx
import onnx
import onnxruntime as rt

input_signature = [tf.TensorSpec([None, 224,224,4], tf.float32, name='x')]
# Use from_function for tf functions
onnx_model, _ = tf2onnx.convert.from_keras(segmentation_model, input_signature, opset=13)
onnx.save(onnx_model, "model.onnx")

session = rt.InferenceSession("model.onnx")
# Get the input tensor's name and shape
input_name = session.get_inputs()[0].name
input_shape = session.get_inputs()[0].shape

# Generate a sample input tensor
input_data = np.expand_dims(X[68], axis=0).astype(np.float32)

# Run inference and get the output tensor
output = session.run(None, {input_name: input_data})[0]
# Post process
result = output[0].squeeze()
plt.imshow(result, cmap=custom_cmap, vmin=0, vmax=1)

#plt.show()